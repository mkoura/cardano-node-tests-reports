{
  "uid" : "c23ca68d5592c1a0",
  "name" : "test_opcert_future_kes_period",
  "fullName" : "cardano_node_tests.tests.test_kes.TestKES#test_opcert_future_kes_period",
  "historyId" : "b8c1571d0266203121c79c0d2d0b161f",
  "time" : {
    "start" : 1658756162188,
    "stop" : 1658757308343,
    "duration" : 1146155
  },
  "description" : "Start a stake pool with an operational certificate created with invalid `--kes-period`.\n\n        * generate new operational certificate with `--kes-period` in the future\n        * restart the node with the new operational certificate\n        * check that the pool is not minting any blocks\n        * if network era > Alonzo\n\n            - generate new operational certificate with valid `--kes-period`, but counter value +2\n              from last used operational certificate\n            - restart the node\n            - check that the pool is not minting any blocks\n\n        * generate new operational certificate with valid `--kes-period` and restart the node\n        * check that the pool is minting blocks again\n        ",
  "descriptionHtml" : "<p>Start a stake pool with an operational certificate created with invalid <code>--kes-period</code>.</p>\n<pre><code>    * generate new operational certificate with `--kes-period` in the future\n    * restart the node with the new operational certificate\n    * check that the pool is not minting any blocks\n    * if network era &gt; Alonzo\n\n        - generate new operational certificate with valid `--kes-period`, but counter value +2\n          from last used operational certificate\n        - restart the node\n        - check that the pool is not minting any blocks\n\n    * generate new operational certificate with valid `--kes-period` and restart the node\n    * check that the pool is minting blocks again\n</code></pre>\n",
  "status" : "skipped",
  "statusMessage" : "XFAIL reason: See cardano-node issue #4114\n\n_pytest.outcomes.XFailed: See cardano-node issue #4114",
  "statusTrace" : "self = <cardano_node_tests.tests.test_kes.TestKES object at 0x7f7fbeb9bcd0>, cluster_lock_pool2 = <ClusterLib: protocol=cardano, tx_era=babbage>\ncluster_manager = <cardano_node_tests.utils.cluster_management.ClusterManager object at 0x7f7fbeb9b520>\n\n    @allure.link(helpers.get_vcs_link())\n    @pytest.mark.order(6)\n    @pytest.mark.long\n    def test_opcert_future_kes_period(  # noqa: C901\n        self,\n        cluster_lock_pool2: clusterlib.ClusterLib,\n        cluster_manager: cluster_management.ClusterManager,\n    ):\n        \"\"\"Start a stake pool with an operational certificate created with invalid `--kes-period`.\n    \n        * generate new operational certificate with `--kes-period` in the future\n        * restart the node with the new operational certificate\n        * check that the pool is not minting any blocks\n        * if network era > Alonzo\n    \n            - generate new operational certificate with valid `--kes-period`, but counter value +2\n              from last used operational certificate\n            - restart the node\n            - check that the pool is not minting any blocks\n    \n        * generate new operational certificate with valid `--kes-period` and restart the node\n        * check that the pool is minting blocks again\n        \"\"\"\n        # pylint: disable=too-many-statements,too-many-branches\n        __: Any  # mypy workaround\n        kes_period_info_errors_list = []\n        pool_name = cluster_management.Resources.POOL2\n        pool_num = 2\n        node_name = \"pool2\"\n        cluster = cluster_lock_pool2\n    \n        temp_template = common.get_test_id(cluster)\n        pool_rec = cluster_manager.cache.addrs_data[pool_name]\n    \n        node_cold = pool_rec[\"cold_key_pair\"]\n        pool_id = cluster.get_stake_pool_id(node_cold.vkey_file)\n        pool_id_dec = helpers.decode_bech32(pool_id)\n    \n        opcert_file: Path = pool_rec[\"pool_operational_cert\"]\n        cold_counter_file: Path = pool_rec[\"cold_key_pair\"].counter_file\n    \n        expected_errors = [\n            (f\"{node_name}.stdout\", \"PraosCannotForgeKeyNotUsableYet\"),\n        ]\n    \n        if VERSIONS.cluster_era > VERSIONS.ALONZO:\n            expected_errors.append((f\"{node_name}.stdout\", \"CounterOverIncrementedOCERT\"))\n            # In Babbage we get `CounterOverIncrementedOCERT` error if counter for new opcert\n            # is not exactly +1 from last used opcert. We'll backup the original counter\n            # file so we can use it for issuing next valid opcert.\n            cold_counter_file_orig = Path(\n                f\"{cold_counter_file.stem}_orig{cold_counter_file.suffix}\"\n            ).resolve()\n            shutil.copy(cold_counter_file, cold_counter_file_orig)\n    \n        logfiles.add_ignore_rule(\n            files_glob=\"*.stdout\",\n            regex=\"MuxBearerClosed|CounterOverIncrementedOCERT\",\n            ignore_file_id=cluster_manager.worker_id,\n        )\n    \n        # generate new operational certificate with `--kes-period` in the future\n        invalid_opcert_file = cluster.gen_node_operational_cert(\n            node_name=f\"{node_name}_invalid_opcert_file\",\n            kes_vkey_file=pool_rec[\"kes_key_pair\"].vkey_file,\n            cold_skey_file=pool_rec[\"cold_key_pair\"].skey_file,\n            cold_counter_file=cold_counter_file,\n            kes_period=cluster.get_kes_period() + 100,\n        )\n    \n        with cluster_manager.restart_on_failure():\n            with logfiles.expect_errors(expected_errors, ignore_file_id=cluster_manager.worker_id):\n                # restart the node with the new operational certificate\n                shutil.copy(invalid_opcert_file, opcert_file)\n                cluster_nodes.restart_nodes([node_name])\n    \n                LOGGER.info(\"Checking blocks production for 4 epochs.\")\n                this_epoch = cluster.get_epoch()\n                for invalid_opcert_epoch in range(4):\n                    this_epoch, is_minting = _check_block_production(\n                        cluster_obj=cluster,\n                        temp_template=temp_template,\n                        pool_id_dec=pool_id_dec,\n                        pool_num=pool_num,\n                        in_epoch=this_epoch + 1,\n                    )\n    \n                    # check that the pool is not minting any blocks\n                    assert (\n                        not is_minting\n                    ), f\"The pool '{pool_name}' has minted blocks in epoch {this_epoch}\"\n    \n                    if invalid_opcert_epoch == 1:\n                        # check kes-period-info with operational certificate with\n                        # invalid `--kes-period`\n                        kes_period_info = cluster.get_kes_period_info(invalid_opcert_file)\n                        kes_period_info_errors_list.append(\n                            kes.check_kes_period_info_result(\n                                kes_output=kes_period_info,\n                                expected_scenario=kes.KesScenarios.INVALID_KES_PERIOD,\n                                check_id=\"1\",\n                            )\n                        )\n    \n                    # test the `CounterOverIncrementedOCERT` error - the counter will now be +2 from\n                    # last used opcert counter value\n                    if invalid_opcert_epoch == 2 and VERSIONS.cluster_era > VERSIONS.ALONZO:\n                        overincrement_opcert_file = cluster.gen_node_operational_cert(\n                            node_name=f\"{node_name}_overincrement_opcert_file\",\n                            kes_vkey_file=pool_rec[\"kes_key_pair\"].vkey_file,\n                            cold_skey_file=pool_rec[\"cold_key_pair\"].skey_file,\n                            cold_counter_file=cold_counter_file,\n                            kes_period=cluster.get_kes_period(),\n                        )\n                        # copy the new certificate and restart the node\n                        shutil.copy(overincrement_opcert_file, opcert_file)\n                        cluster_nodes.restart_nodes([node_name])\n    \n                        kes_period_info = cluster.get_kes_period_info(overincrement_opcert_file)\n                        kes_period_info_errors_list.append(\n                            kes.check_kes_period_info_result(\n                                kes_output=kes_period_info,\n                                expected_scenario=kes.KesScenarios.INVALID_COUNTERS,\n                                check_id=\"2\",\n                            )\n                        )\n    \n                    if invalid_opcert_epoch == 3:\n                        # check kes-period-info with operational certificate with\n                        # invalid kes-period\n                        kes_period_info = cluster.get_kes_period_info(invalid_opcert_file)\n                        kes_period_info_errors_list.append(\n                            kes.check_kes_period_info_result(\n                                kes_output=kes_period_info,\n                                expected_scenario=kes.KesScenarios.INVALID_KES_PERIOD,\n                                check_id=\"3\",\n                            )\n                        )\n    \n            # in Babbage we'll use the original counter for issuing new valid opcert so the counter\n            # value of new valid opcert equals to counter value of the original opcert +1\n            if VERSIONS.cluster_era > VERSIONS.ALONZO:\n                shutil.copy(cold_counter_file_orig, cold_counter_file)\n    \n            # generate new operational certificate with valid `--kes-period`\n            valid_opcert_file = cluster.gen_node_operational_cert(\n                node_name=f\"{node_name}_valid_opcert_file\",\n                kes_vkey_file=pool_rec[\"kes_key_pair\"].vkey_file,\n                cold_skey_file=pool_rec[\"cold_key_pair\"].skey_file,\n                cold_counter_file=cold_counter_file,\n                kes_period=cluster.get_kes_period(),\n            )\n            # copy the new certificate and restart the node\n            shutil.copy(valid_opcert_file, opcert_file)\n            cluster_nodes.restart_nodes([node_name])\n    \n            LOGGER.info(\"Checking blocks production for up to 6 epochs.\")\n            updated_epoch = cluster.get_epoch()\n            this_epoch = updated_epoch\n            for __ in range(6):\n                this_epoch, is_minting = _check_block_production(\n                    cluster_obj=cluster,\n                    temp_template=temp_template,\n                    pool_id_dec=pool_id_dec,\n                    pool_num=pool_num,\n                    in_epoch=this_epoch + 1,\n                )\n    \n                # check that the pool is minting blocks\n                if is_minting:\n                    break\n            else:\n                raise AssertionError(\n                    f\"The pool '{pool_name}' has not minted any blocks since epoch {updated_epoch}.\"\n                )\n    \n        # check kes-period-info with valid operational certificate\n        kes_period_info = cluster.get_kes_period_info(valid_opcert_file)\n        kes_period_info_errors_list.append(\n            kes.check_kes_period_info_result(\n                kes_output=kes_period_info,\n                expected_scenario=kes.KesScenarios.ALL_VALID,\n                check_id=\"4\",\n            )\n        )\n    \n        # check kes-period-info with operational certificate with invalid counter and kes-period\n        kes_period_info = cluster.get_kes_period_info(invalid_opcert_file)\n        kes_period_info_errors_list.append(\n            kes.check_kes_period_info_result(\n                kes_output=kes_period_info,\n                expected_scenario=kes.KesScenarios.INVALID_KES_PERIOD\n                if VERSIONS.cluster_era > VERSIONS.ALONZO\n                else kes.KesScenarios.ALL_INVALID,\n                check_id=\"5\",\n            )\n        )\n    \n        clean_errors_list = [e for e in kes_period_info_errors_list if e]\n        err_joined = \"\\n\".join(clean_errors_list)\n    \n        if err_joined:\n            if (\n                VERSIONS.cluster_era > VERSIONS.ALONZO\n                and len(clean_errors_list) == 1\n                and \"check '2'\" in err_joined\n            ):\n>               pytest.xfail(\"See cardano-node issue #4114\")\nE               _pytest.outcomes.XFailed: See cardano-node issue #4114\n\n/home/martink/Source/repos/cardano-node-tests2/cardano_node_tests/tests/test_kes.py:510: XFailed",
  "flaky" : false,
  "newFailed" : false,
  "beforeStages" : [ {
    "name" : "cluster_manager",
    "time" : {
      "start" : 1658753120855,
      "stop" : 1658753120855,
      "duration" : 0
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "cd_testfile_temp_dir",
    "time" : {
      "start" : 1658753120854,
      "stop" : 1658753120855,
      "duration" : 1
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "testenv_setup_teardown",
    "time" : {
      "start" : 1658753120502,
      "stop" : 1658753120854,
      "duration" : 352
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "worker_id",
    "time" : {
      "start" : 1658753120502,
      "stop" : 1658753120502,
      "duration" : 0
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "close_dbconn",
    "time" : {
      "start" : 1658753120502,
      "stop" : 1658753120502,
      "duration" : 0
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "cluster_lock_pool2",
    "time" : {
      "start" : 1658753120855,
      "stop" : 1658756162187,
      "duration" : 3041332
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "function_autouse",
    "time" : {
      "start" : 1658753120855,
      "stop" : 1658753120855,
      "duration" : 0
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "tmp_path_factory",
    "time" : {
      "start" : 1658753120501,
      "stop" : 1658753120501,
      "duration" : 0
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "change_dir",
    "time" : {
      "start" : 1658753120501,
      "stop" : 1658753120502,
      "duration" : 1
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "session_autouse",
    "time" : {
      "start" : 1658753120854,
      "stop" : 1658753120854,
      "duration" : 0
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "testfile_temp_dir",
    "time" : {
      "start" : 1658753120854,
      "stop" : 1658753120854,
      "duration" : 0
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  } ],
  "testStage" : {
    "description" : "Start a stake pool with an operational certificate created with invalid `--kes-period`.\n\n        * generate new operational certificate with `--kes-period` in the future\n        * restart the node with the new operational certificate\n        * check that the pool is not minting any blocks\n        * if network era > Alonzo\n\n            - generate new operational certificate with valid `--kes-period`, but counter value +2\n              from last used operational certificate\n            - restart the node\n            - check that the pool is not minting any blocks\n\n        * generate new operational certificate with valid `--kes-period` and restart the node\n        * check that the pool is minting blocks again\n        ",
    "status" : "skipped",
    "statusMessage" : "XFAIL reason: See cardano-node issue #4114\n\n_pytest.outcomes.XFailed: See cardano-node issue #4114",
    "statusTrace" : "self = <cardano_node_tests.tests.test_kes.TestKES object at 0x7f7fbeb9bcd0>, cluster_lock_pool2 = <ClusterLib: protocol=cardano, tx_era=babbage>\ncluster_manager = <cardano_node_tests.utils.cluster_management.ClusterManager object at 0x7f7fbeb9b520>\n\n    @allure.link(helpers.get_vcs_link())\n    @pytest.mark.order(6)\n    @pytest.mark.long\n    def test_opcert_future_kes_period(  # noqa: C901\n        self,\n        cluster_lock_pool2: clusterlib.ClusterLib,\n        cluster_manager: cluster_management.ClusterManager,\n    ):\n        \"\"\"Start a stake pool with an operational certificate created with invalid `--kes-period`.\n    \n        * generate new operational certificate with `--kes-period` in the future\n        * restart the node with the new operational certificate\n        * check that the pool is not minting any blocks\n        * if network era > Alonzo\n    \n            - generate new operational certificate with valid `--kes-period`, but counter value +2\n              from last used operational certificate\n            - restart the node\n            - check that the pool is not minting any blocks\n    \n        * generate new operational certificate with valid `--kes-period` and restart the node\n        * check that the pool is minting blocks again\n        \"\"\"\n        # pylint: disable=too-many-statements,too-many-branches\n        __: Any  # mypy workaround\n        kes_period_info_errors_list = []\n        pool_name = cluster_management.Resources.POOL2\n        pool_num = 2\n        node_name = \"pool2\"\n        cluster = cluster_lock_pool2\n    \n        temp_template = common.get_test_id(cluster)\n        pool_rec = cluster_manager.cache.addrs_data[pool_name]\n    \n        node_cold = pool_rec[\"cold_key_pair\"]\n        pool_id = cluster.get_stake_pool_id(node_cold.vkey_file)\n        pool_id_dec = helpers.decode_bech32(pool_id)\n    \n        opcert_file: Path = pool_rec[\"pool_operational_cert\"]\n        cold_counter_file: Path = pool_rec[\"cold_key_pair\"].counter_file\n    \n        expected_errors = [\n            (f\"{node_name}.stdout\", \"PraosCannotForgeKeyNotUsableYet\"),\n        ]\n    \n        if VERSIONS.cluster_era > VERSIONS.ALONZO:\n            expected_errors.append((f\"{node_name}.stdout\", \"CounterOverIncrementedOCERT\"))\n            # In Babbage we get `CounterOverIncrementedOCERT` error if counter for new opcert\n            # is not exactly +1 from last used opcert. We'll backup the original counter\n            # file so we can use it for issuing next valid opcert.\n            cold_counter_file_orig = Path(\n                f\"{cold_counter_file.stem}_orig{cold_counter_file.suffix}\"\n            ).resolve()\n            shutil.copy(cold_counter_file, cold_counter_file_orig)\n    \n        logfiles.add_ignore_rule(\n            files_glob=\"*.stdout\",\n            regex=\"MuxBearerClosed|CounterOverIncrementedOCERT\",\n            ignore_file_id=cluster_manager.worker_id,\n        )\n    \n        # generate new operational certificate with `--kes-period` in the future\n        invalid_opcert_file = cluster.gen_node_operational_cert(\n            node_name=f\"{node_name}_invalid_opcert_file\",\n            kes_vkey_file=pool_rec[\"kes_key_pair\"].vkey_file,\n            cold_skey_file=pool_rec[\"cold_key_pair\"].skey_file,\n            cold_counter_file=cold_counter_file,\n            kes_period=cluster.get_kes_period() + 100,\n        )\n    \n        with cluster_manager.restart_on_failure():\n            with logfiles.expect_errors(expected_errors, ignore_file_id=cluster_manager.worker_id):\n                # restart the node with the new operational certificate\n                shutil.copy(invalid_opcert_file, opcert_file)\n                cluster_nodes.restart_nodes([node_name])\n    \n                LOGGER.info(\"Checking blocks production for 4 epochs.\")\n                this_epoch = cluster.get_epoch()\n                for invalid_opcert_epoch in range(4):\n                    this_epoch, is_minting = _check_block_production(\n                        cluster_obj=cluster,\n                        temp_template=temp_template,\n                        pool_id_dec=pool_id_dec,\n                        pool_num=pool_num,\n                        in_epoch=this_epoch + 1,\n                    )\n    \n                    # check that the pool is not minting any blocks\n                    assert (\n                        not is_minting\n                    ), f\"The pool '{pool_name}' has minted blocks in epoch {this_epoch}\"\n    \n                    if invalid_opcert_epoch == 1:\n                        # check kes-period-info with operational certificate with\n                        # invalid `--kes-period`\n                        kes_period_info = cluster.get_kes_period_info(invalid_opcert_file)\n                        kes_period_info_errors_list.append(\n                            kes.check_kes_period_info_result(\n                                kes_output=kes_period_info,\n                                expected_scenario=kes.KesScenarios.INVALID_KES_PERIOD,\n                                check_id=\"1\",\n                            )\n                        )\n    \n                    # test the `CounterOverIncrementedOCERT` error - the counter will now be +2 from\n                    # last used opcert counter value\n                    if invalid_opcert_epoch == 2 and VERSIONS.cluster_era > VERSIONS.ALONZO:\n                        overincrement_opcert_file = cluster.gen_node_operational_cert(\n                            node_name=f\"{node_name}_overincrement_opcert_file\",\n                            kes_vkey_file=pool_rec[\"kes_key_pair\"].vkey_file,\n                            cold_skey_file=pool_rec[\"cold_key_pair\"].skey_file,\n                            cold_counter_file=cold_counter_file,\n                            kes_period=cluster.get_kes_period(),\n                        )\n                        # copy the new certificate and restart the node\n                        shutil.copy(overincrement_opcert_file, opcert_file)\n                        cluster_nodes.restart_nodes([node_name])\n    \n                        kes_period_info = cluster.get_kes_period_info(overincrement_opcert_file)\n                        kes_period_info_errors_list.append(\n                            kes.check_kes_period_info_result(\n                                kes_output=kes_period_info,\n                                expected_scenario=kes.KesScenarios.INVALID_COUNTERS,\n                                check_id=\"2\",\n                            )\n                        )\n    \n                    if invalid_opcert_epoch == 3:\n                        # check kes-period-info with operational certificate with\n                        # invalid kes-period\n                        kes_period_info = cluster.get_kes_period_info(invalid_opcert_file)\n                        kes_period_info_errors_list.append(\n                            kes.check_kes_period_info_result(\n                                kes_output=kes_period_info,\n                                expected_scenario=kes.KesScenarios.INVALID_KES_PERIOD,\n                                check_id=\"3\",\n                            )\n                        )\n    \n            # in Babbage we'll use the original counter for issuing new valid opcert so the counter\n            # value of new valid opcert equals to counter value of the original opcert +1\n            if VERSIONS.cluster_era > VERSIONS.ALONZO:\n                shutil.copy(cold_counter_file_orig, cold_counter_file)\n    \n            # generate new operational certificate with valid `--kes-period`\n            valid_opcert_file = cluster.gen_node_operational_cert(\n                node_name=f\"{node_name}_valid_opcert_file\",\n                kes_vkey_file=pool_rec[\"kes_key_pair\"].vkey_file,\n                cold_skey_file=pool_rec[\"cold_key_pair\"].skey_file,\n                cold_counter_file=cold_counter_file,\n                kes_period=cluster.get_kes_period(),\n            )\n            # copy the new certificate and restart the node\n            shutil.copy(valid_opcert_file, opcert_file)\n            cluster_nodes.restart_nodes([node_name])\n    \n            LOGGER.info(\"Checking blocks production for up to 6 epochs.\")\n            updated_epoch = cluster.get_epoch()\n            this_epoch = updated_epoch\n            for __ in range(6):\n                this_epoch, is_minting = _check_block_production(\n                    cluster_obj=cluster,\n                    temp_template=temp_template,\n                    pool_id_dec=pool_id_dec,\n                    pool_num=pool_num,\n                    in_epoch=this_epoch + 1,\n                )\n    \n                # check that the pool is minting blocks\n                if is_minting:\n                    break\n            else:\n                raise AssertionError(\n                    f\"The pool '{pool_name}' has not minted any blocks since epoch {updated_epoch}.\"\n                )\n    \n        # check kes-period-info with valid operational certificate\n        kes_period_info = cluster.get_kes_period_info(valid_opcert_file)\n        kes_period_info_errors_list.append(\n            kes.check_kes_period_info_result(\n                kes_output=kes_period_info,\n                expected_scenario=kes.KesScenarios.ALL_VALID,\n                check_id=\"4\",\n            )\n        )\n    \n        # check kes-period-info with operational certificate with invalid counter and kes-period\n        kes_period_info = cluster.get_kes_period_info(invalid_opcert_file)\n        kes_period_info_errors_list.append(\n            kes.check_kes_period_info_result(\n                kes_output=kes_period_info,\n                expected_scenario=kes.KesScenarios.INVALID_KES_PERIOD\n                if VERSIONS.cluster_era > VERSIONS.ALONZO\n                else kes.KesScenarios.ALL_INVALID,\n                check_id=\"5\",\n            )\n        )\n    \n        clean_errors_list = [e for e in kes_period_info_errors_list if e]\n        err_joined = \"\\n\".join(clean_errors_list)\n    \n        if err_joined:\n            if (\n                VERSIONS.cluster_era > VERSIONS.ALONZO\n                and len(clean_errors_list) == 1\n                and \"check '2'\" in err_joined\n            ):\n>               pytest.xfail(\"See cardano-node issue #4114\")\nE               _pytest.outcomes.XFailed: See cardano-node issue #4114\n\n/home/martink/Source/repos/cardano-node-tests2/cardano_node_tests/tests/test_kes.py:510: XFailed",
    "steps" : [ ],
    "attachments" : [ {
      "uid" : "fac96624a2645fa",
      "name" : "log",
      "source" : "fac96624a2645fa.txt",
      "type" : "text/plain",
      "size" : 2506
    } ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 1,
    "shouldDisplayMessage" : true,
    "hasContent" : true
  },
  "afterStages" : [ {
    "name" : "cluster_manager::0",
    "time" : {
      "start" : 1658757308383,
      "stop" : 1658757308672,
      "duration" : 289
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "cd_testfile_temp_dir::0",
    "time" : {
      "start" : 1658757308673,
      "stop" : 1658757308673,
      "duration" : 0
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "testenv_setup_teardown::0",
    "time" : {
      "start" : 1658757999487,
      "stop" : 1658757999490,
      "duration" : 3
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  }, {
    "name" : "close_dbconn::0",
    "time" : {
      "start" : 1658757999491,
      "stop" : 1658757999491,
      "duration" : 0
    },
    "status" : "passed",
    "steps" : [ ],
    "attachments" : [ ],
    "parameters" : [ ],
    "stepsCount" : 0,
    "attachmentsCount" : 0,
    "shouldDisplayMessage" : false,
    "hasContent" : false
  } ],
  "labels" : [ {
    "name" : "tag",
    "value" : "long"
  }, {
    "name" : "tag",
    "value" : "order(6)"
  }, {
    "name" : "tag",
    "value" : "@pytest.mark.skipif(False, reason='meant to run with default era or higher, where cluster era == Tx era')"
  }, {
    "name" : "parentSuite",
    "value" : "cardano_node_tests.tests"
  }, {
    "name" : "suite",
    "value" : "test_kes"
  }, {
    "name" : "subSuite",
    "value" : "TestKES"
  }, {
    "name" : "host",
    "value" : "bender-3900x"
  }, {
    "name" : "thread",
    "value" : "1995838-MainThread"
  }, {
    "name" : "framework",
    "value" : "pytest"
  }, {
    "name" : "language",
    "value" : "cpython3"
  }, {
    "name" : "package",
    "value" : "cardano_node_tests.tests.test_kes"
  }, {
    "name" : "resultFormat",
    "value" : "allure2"
  } ],
  "parameters" : [ ],
  "links" : [ {
    "name" : "https://github.com/input-output-hk/cardano-node-tests/blob/ed48848e8881877ef3425b7a9c3fa5bb9fb2ced8/cardano_node_tests/tests/test_kes.py#L303",
    "url" : "https://github.com/input-output-hk/cardano-node-tests/blob/ed48848e8881877ef3425b7a9c3fa5bb9fb2ced8/cardano_node_tests/tests/test_kes.py#L303",
    "type" : "link"
  } ],
  "hidden" : false,
  "retry" : false,
  "extra" : {
    "severity" : "normal",
    "retries" : [ {
      "uid" : "bc75c749e51cb506",
      "status" : "skipped",
      "statusDetails" : "Skipped: collected, not run",
      "time" : {
        "start" : 1658753116297,
        "stop" : 1658753116297,
        "duration" : 0
      }
    } ],
    "categories" : [ ],
    "tags" : [ "order(6)", "@pytest.mark.skipif(False, reason='meant to run with default era or higher, where cluster era == Tx era')", "long" ]
  },
  "source" : "c23ca68d5592c1a0.json",
  "parameterValues" : [ ]
}